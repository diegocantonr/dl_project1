{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aux Loss\n",
    "\n",
    "So far, re-using what has worked from previous exploration.\n",
    "Basic AuxLoss based on the weight-sharing network. Will have to redo another auxLoss without the weight-sharing (not sure how to implement that...)\n",
    "\n",
    "Haven't done anything like data augmentation&randomization yet, just normalization, batchnorm, all using SGD. Maybe worth to try Adam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue as prologue\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_(train_input,test_input):\n",
    "    \"\"\"Function to normalize the input\"\"\"\n",
    "    mu, std = train_input.mean(), train_input.std()\n",
    "    train_inputOut = train_input.sub_(mu).div_(std)\n",
    "    test_inputOut = test_input.sub_(mu).div_(std)\n",
    "    return train_inputOut, test_inputOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Generating pairs of 14x14\n",
    "N=1000\n",
    "(train_input,train_target,train_classes, \\\n",
    " test_input,test_target,test_classes) = prologue.generate_pair_sets(N)\n",
    "\n",
    "train_input = train_input.to(device)\n",
    "test_input = test_input.to(device)\n",
    "train_target = train_target.to(device)\n",
    "test_target = test_target.to(device)\n",
    "train_classes, test_classes = train_classes.to(device), test_classes.to(device)\n",
    "\n",
    "train_input,test_input = norm_(train_input,test_input);\n",
    "print(train_input.device)\n",
    "print(train_classes.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(test_classes.narrow(1,0,1).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model \n",
    "Weight-shared convnet, 2 conv layers, 3 hidden layers, 1 output final layer with softmax as last non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Conv layers, based on the filter size and layer sizes tested in explorationLeNetDual\n",
    "# must take 1x14x14 (so the same layers is used on both images)\n",
    "# Separated the modules because I couldn't figure out how to make a single net work...\n",
    "# will maybe merge into a single net later.\n",
    "class convlayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convlayer, self).__init__()\n",
    "        #self.conv1 : takes 1x14x14, gives 32x12x12, then maxpool(k=2) -> 32x6x6\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3) \n",
    "        #self.conv2 : takes 32x6x6, gives 64x4x4, then maxpool(k=2) -> outputs 64x2x2 to the fc layers\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2,stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2,stride=2))\n",
    "        return x\n",
    "#fc layers, adding a supp layer that has output dim 10 (instead of 2)\n",
    "#in order to maybe calculate an aux loss on this output to have classification?\n",
    "# REMEMBER TO ADD AN ACTIVATION WHEN CALLING SHARED_FCLAYER IN YOUR NETWORK!!\n",
    "# ex : F.relu(self.shared_fclayer(tmp1) or F.softmax(...,dim=1)\n",
    "class shared_fclayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(shared_fclayer,self).__init__()\n",
    "        #gets in 64x2x2, convers to 1x250\n",
    "        self.fc1 = nn.Linear(2*2*64,264)\n",
    "        self.bn1 = nn.BatchNorm1d(264)\n",
    "        #second layer : 250 to 100\n",
    "        self.fc2 = nn.Linear(264,100)  \n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "        #outputs dim 10 so we can test the aux loss for classifying numbers\n",
    "        #use softmax on fc3?\n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    def forward(self,x):\n",
    "        x = self.dropout(self.bn1(F.relu(self.fc1(x.view(-1,2*2*64)))))\n",
    "        x = self.dropout(self.bn2(F.relu(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "#extra final layer, not shared\n",
    "class final_predictionlayer(nn.Module):\n",
    "    #this final layer should take 2*10 (one for each image) and output 2 \n",
    "    def __init__(self):\n",
    "        super(final_predictionlayer,self).__init__()\n",
    "        self.final = nn.Linear(20,2)\n",
    "    def forward(self,x):\n",
    "        x = F.softmax(self.final(x),dim=1)\n",
    "        return x\n",
    "\n",
    "#weight-sharing Net\n",
    "#returns tmp1, tmp2 in order to calculate and optimize with auxLoss\n",
    "#Those will be compared to the train_classes.narrow(1,0,1) and .narrow(1,1,1)\n",
    "class ws_Net_normal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ws_Net_normal,self).__init__()\n",
    "        self.convlayer = convlayer()\n",
    "        self.fclayer = shared_fclayer()\n",
    "        self.final = final_predictionlayer()\n",
    "    def forward(self,x):\n",
    "        tmp1 = x.narrow(1,0,1) #viewing only one image\n",
    "        tmp2 = x.narrow(1,1,1) #viewing only one image\n",
    "        #applying the conv layers\n",
    "        tmp1 = self.convlayer.forward(tmp1) \n",
    "        tmp2 = self.convlayer.forward(tmp2)\n",
    "        #applying the fc layers\n",
    "        tmp1 = F.relu(self.fclayer(tmp1))\n",
    "        tmp2 = F.relu(self.fclayer(tmp2))\n",
    "        #viewing and final prediction\n",
    "        output = torch.cat((tmp1,tmp2),1)\n",
    "        output.view(-1,20)\n",
    "        x = self.final(output)\n",
    "        return x, tmp1, tmp2\n",
    "\n",
    "class ws_Net_softmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ws_Net_softmax,self).__init__()\n",
    "        self.convlayer = convlayer()\n",
    "        self.fclayer = shared_fclayer()\n",
    "        self.final = final_predictionlayer()\n",
    "    def forward(self,x):\n",
    "        tmp1 = x.narrow(1,0,1) #viewing only one image\n",
    "        tmp2 = x.narrow(1,1,1) #viewing only one image\n",
    "        #applying the conv layers\n",
    "        tmp1 = self.convlayer.forward(tmp1) \n",
    "        tmp2 = self.convlayer.forward(tmp2)\n",
    "        #applying the fc layers\n",
    "        tmp1 = F.softmax(self.fclayer(tmp1),dim=1)\n",
    "        tmp2 = F.softmax(self.fclayer(tmp2),dim=1)\n",
    "        #viewing and final prediction\n",
    "        output = torch.cat((tmp1,tmp2),1)\n",
    "        output.view(-1,20)\n",
    "        x = self.final(output)\n",
    "        return x, tmp1, tmp2\n",
    "\n",
    "class ws_Net_double(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ws_Net_double,self).__init__()\n",
    "        self.convlayer = convlayer()\n",
    "        self.fclayer = shared_fclayer()\n",
    "        self.final = final_predictionlayer()\n",
    "    def forward(self,x):\n",
    "        tmp1 = x.narrow(1,0,1) #viewing only one image\n",
    "        tmp2 = x.narrow(1,1,1) #viewing only one image\n",
    "        #applying the conv layers\n",
    "        tmp1 = self.convlayer.forward(tmp1) \n",
    "        tmp2 = self.convlayer.forward(tmp2)\n",
    "        #applying the fc layers\n",
    "        tmp1 = F.relu(self.fclayer(tmp1))\n",
    "        tmp2 = F.relu(self.fclayer(tmp2))\n",
    "        #viewing and final prediction\n",
    "        output = torch.cat((tmp1,tmp2),1)\n",
    "        output.view(-1,20)\n",
    "        x = self.final(output)\n",
    "        tmp1 = F.softmax(tmp1,dim=1)\n",
    "        tmp2 = F.softmax(tmp2,dim=1)\n",
    "        return x, tmp1, tmp2\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train_model\n",
    "Has to be adapted to optimize for auxloss also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_aux(model, train_input, train_target, train_classes, nb_epochs=50, \n",
    "                eta=9e-2, mini_batch_size=25, gamma = 0.33, printTrain = False, graphLoss = False):\n",
    "    \"\"\"Trains the model, using CrossEntropyLoss and SGD \n",
    "    Model : Architecture to be tested, pytorch.nn.Module\n",
    "    Train_input : Input tensors Nx2x14x14, N = 1000\n",
    "    Train_target : Target labels, N, classes = 0 or 1\n",
    "    Nb_epochs : nb of epochs to train over\n",
    "    eta : Learning rate\n",
    "    mini_batch_size : Size of minibatch to be processed\"\"\"\n",
    "    \n",
    "    #Squeeze the classes labels (hotlabeling)\n",
    "    trainlabel_1 = (train_classes.narrow(1,0,1)).squeeze()\n",
    "    trainlabel_2 = (train_classes.narrow(1,1,1)).squeeze()\n",
    "\n",
    "    model.train(True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(),lr=eta)\n",
    "    losses = []\n",
    "    for e in range(nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):    \n",
    "            \n",
    "            out_compare, out_1, out_2 = model(train_input.narrow(0, b, mini_batch_size))\n",
    "                \n",
    "            loss_compare = criterion(out_compare, train_target.narrow(0, b, mini_batch_size))\n",
    "            loss_1 = criterion(out_1, trainlabel_1.narrow(0, b, mini_batch_size))\n",
    "            loss_2 = criterion(out_2, trainlabel_2.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            loss_sum = loss_1+loss_2 + gamma*loss_compare\n",
    "            \n",
    "            losses.append(loss_sum)\n",
    "            model.zero_grad()\n",
    "            loss_sum.backward()\n",
    "            optimizer.step()\n",
    "        if printTrain : \n",
    "            print(\"Epoch : {} :: Train error : {}/{}, {:0f}%\".format(e,\n",
    "            compute_nb_errors(model,train_input,train_target,mini_batch_size),train_target.size(0),\n",
    "            (100*compute_nb_errors(model,train_input,train_target,mini_batch_size)/train_target.size(0))))\n",
    "    if graphLoss : \n",
    "        plt.plot(losses)\n",
    "        plt.ylabel('loss')\n",
    "#--------------------------------------------------------------------------------------------------------#\n",
    "def compute_nb_errors(model,data_input,data_target,mini_batch_size):\n",
    "    \"\"\"std from the s√©ries\"\"\"\n",
    "    nb_errors = 0;\n",
    "    model.to(device)\n",
    "    data_input, data_target = data_input.to(device),data_target.to(device)\n",
    "    for b in range(0,data_input.size(0),mini_batch_size):\n",
    "        output, _, _ = model(data_input.narrow(0,b,mini_batch_size))\n",
    "        _, predicted_classes = torch.max(output, 1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b+k]!=predicted_classes[k]:\n",
    "                nb_errors += 1\n",
    "    \n",
    "    return nb_errors\n",
    "#--------------------------------------------------------------------------------------------------------#\n",
    "def run_net_aux(model,train_input,train_target, train_classes,\n",
    "            test_input, test_target, test_classes,\n",
    "            nb_epochs = 50,eta=9e-2,mini_batch_size=25,gamma=0.33,\n",
    "            printTrain = False,graphLoss = False):\n",
    "    \"\"\"\"\"\"\n",
    "    model.to(device)\n",
    "    print(\"Model tested : {}\".format(str(model)[:str(model).find('(')]))\n",
    "    print(\"\"\"Using {} epochs, lr = {:.04f},Mini batch size = {}\"\"\".format(nb_epochs,\n",
    "                                                                          eta,mini_batch_size))\n",
    "    train_model_aux(model, train_input, train_target, train_classes,\n",
    "                nb_epochs, eta, mini_batch_size, gamma, printTrain,graphLoss)\n",
    "    model.train(False)\n",
    "    train_error = compute_nb_errors(model, train_input, train_target,mini_batch_size) / train_input.size(0) * 100\n",
    "    test_error = compute_nb_errors(model, test_input, test_target,mini_batch_size) / test_input.size(0) * 100\n",
    "    print('train_error {:.02f}% test_error {:.02f}% \\n'.format(\n",
    "                train_error,\n",
    "                test_error\n",
    "            )\n",
    "        )\n",
    "    return float(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tested : ws_Net\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-2945a509813a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     err += run_net_aux(ws_Net(), train_input, train_target, train_classes,\n\u001b[1;32m----> 7\u001b[1;33m                 test_input, test_target, test_classes,gamma = gamma)\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average error for gamma = {}, over {} runs :: {:02f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-89cc35cb63d1>\u001b[0m in \u001b[0;36mrun_net_aux\u001b[1;34m(model, train_input, train_target, train_classes, test_input, test_target, test_classes, nb_epochs, eta, mini_batch_size, gamma, printTrain, graphLoss)\u001b[0m\n\u001b[0;32m     64\u001b[0m                                                                           eta,mini_batch_size))\n\u001b[0;32m     65\u001b[0m     train_model_aux(model, train_input, train_target, train_classes,\n\u001b[1;32m---> 66\u001b[1;33m                 nb_epochs, eta, mini_batch_size, gamma, printTrain,graphLoss)\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mtrain_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_nb_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-89cc35cb63d1>\u001b[0m in \u001b[0;36mtrain_model_aux\u001b[1;34m(model, train_input, train_target, train_classes, nb_epochs, eta, mini_batch_size, gamma, printTrain, graphLoss)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mloss_sum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprintTrain\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\richi\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\richi\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "err = 0.0;\n",
    "nb = 5\n",
    "gamma = 0.67\n",
    "#Testing with Softmax after RELU on TMP1,2\n",
    "for n in range(nb):\n",
    "    err += run_net_aux(ws_Net(), train_input, train_target, train_classes,\n",
    "                test_input, test_target, test_classes,gamma = gamma)\n",
    "print(\"Average error for gamma = {}, over {} runs :: {:02f}\".format(gamma,nb,err/nb))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tested : ws_Net_normal\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 1.30% test_error 10.30% \n",
      "\n",
      "Model tested : ws_Net_normal\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 0.60% test_error 8.30% \n",
      "\n",
      "Model tested : ws_Net_normal\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 0.60% test_error 8.40% \n",
      "\n",
      "Model tested : ws_Net_normal\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 0.50% test_error 8.30% \n",
      "\n",
      "Model tested : ws_Net_normal\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 0.50% test_error 8.10% \n",
      "\n",
      "Normal Net : ReLU on tmp1, tmp2\n",
      "Average error for gamma = 0.67, over 5 runs :: 8.680000\n"
     ]
    }
   ],
   "source": [
    "err = 0.0;\n",
    "nb = 5\n",
    "gamma = 0.67\n",
    "#Testing with relu on fully_connected (on linear(100,10)) on TMP1,2\n",
    "for n in range(nb):\n",
    "    err += run_net_aux(ws_Net_normal(), train_input, train_target, train_classes,\n",
    "                test_input, test_target, test_classes,gamma = gamma)\n",
    "print(\"Normal Net : ReLU on tmp1, tmp2\")\n",
    "print(\"Average error for gamma = {}, over {} runs :: {:02f}\".format(gamma,nb,err/nb))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tested : ws_Net_softmax\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 1.80% test_error 4.90% \n",
      "\n",
      "Model tested : ws_Net_softmax\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 2.40% test_error 5.10% \n",
      "\n",
      "Model tested : ws_Net_softmax\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 2.80% test_error 5.90% \n",
      "\n",
      "Model tested : ws_Net_softmax\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 3.20% test_error 6.30% \n",
      "\n",
      "Model tested : ws_Net_softmax\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 2.60% test_error 7.30% \n",
      "\n",
      "Softmax Net : Softmax only on tmp1, tmp2 NO RELU AT ALL ON FC3\n",
      "Average error for gamma = 0.67, over 5 runs :: 5.900000\n"
     ]
    }
   ],
   "source": [
    "err = 0.0;\n",
    "nb = 5\n",
    "gamma = 0.67\n",
    "#Testing with softmax on fully_connected (on linear(100,10)) on TMP1,2\n",
    "for n in range(nb):\n",
    "    err += run_net_aux(ws_Net_softmax(), train_input, train_target, train_classes,\n",
    "                test_input, test_target, test_classes,gamma = gamma)\n",
    "print(\"Softmax Net : Softmax only on tmp1, tmp2 NO RELU AT ALL ON FC3\")\n",
    "print(\"Average error for gamma = {}, over {} runs :: {:02f}\".format(gamma,nb,err/nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tested : ws_Net_double\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 0.30% test_error 8.40% \n",
      "\n",
      "Model tested : ws_Net_double\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 0.30% test_error 8.10% \n",
      "\n",
      "Model tested : ws_Net_double\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 0.40% test_error 7.40% \n",
      "\n",
      "Model tested : ws_Net_double\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 0.10% test_error 8.00% \n",
      "\n",
      "Model tested : ws_Net_double\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 0.20% test_error 9.00% \n",
      "\n",
      "Double Net : Relu on tmp1, tmp2 after fc, into output, then Softmax on tmp1, tmp2\n",
      "Average error for gamma = 0.67, over 5 runs :: 8.180000%\n"
     ]
    }
   ],
   "source": [
    "err = 0.0;\n",
    "nb = 5\n",
    "gamma = 0.67\n",
    "#Testing with softmax on relu on fully_connected (on linear(100,10)) on TMP1,2\n",
    "for n in range(nb):\n",
    "    err += run_net_aux(ws_Net_double(), train_input, train_target, train_classes,\n",
    "                test_input, test_target, test_classes,gamma = gamma)\n",
    "\n",
    "print(\"Double Net : Relu on tmp1, tmp2 after fc, into output, then Softmax on tmp1, tmp2\")\n",
    "print(\"Average error for gamma = {}, over {} runs :: {:02f}%\".format(gamma,nb,err/nb))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tested : ws_Net_softmax\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 1.80% test_error 5.20% \n",
      "\n",
      "Model tested : ws_Net_softmax\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 2.80% test_error 6.40% \n",
      "\n",
      "Model tested : ws_Net_softmax\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 3.50% test_error 6.10% \n",
      "\n",
      "Model tested : ws_Net_softmax\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 4.40% test_error 7.30% \n",
      "\n",
      "Model tested : ws_Net_softmax\n",
      "Using 50 epochs, lr = 0.0900,Mini batch size = 25\n",
      "train_error 2.80% test_error 5.80% \n",
      "\n",
      "Softmax Net : Softmax only on tmp1, tmp2 NO RELU AT ALL ON FC3\n",
      "Average error for gamma = 0.67, over 5 runs :: 6.160000%\n"
     ]
    }
   ],
   "source": [
    "err = 0.0;\n",
    "nb = 5\n",
    "gamma = 0.67\n",
    "#Testing with softmax on fully_connected (on linear(100,10)) on TMP1,2\n",
    "for n in range(nb):\n",
    "    err += run_net_aux(ws_Net_softmax(), train_input, train_target, train_classes,\n",
    "                test_input, test_target, test_classes,gamma = gamma)\n",
    "print(\"Softmax Net : Softmax only on tmp1, tmp2 NO RELU AT ALL ON FC3\")\n",
    "print(\"Average error for gamma = {}, over {} runs :: {:f}%\".format(gamma,nb,err/nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
