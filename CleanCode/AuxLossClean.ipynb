{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned up basic LeNet exploration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Starting points : Importing, getting data, normalizing*\n",
    "`Stuff you need, but is hidden here.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using : cuda\n"
     ]
    }
   ],
   "source": [
    "#Starting points : import everything, use cuda if available\n",
    "import torch\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue as prologue\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using : {}\".format(device))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using : {}\".format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def norm_(train_input,test_input):\n",
    "    \"\"\"Function to normalize the input --> done IN PLACE!\"\"\"\n",
    "    mu, std = train_input.mean(), train_input.std()\n",
    "    train_inputOut = train_input.sub_(mu).div_(std)\n",
    "    test_inputOut = test_input.sub_(mu).div_(std)\n",
    "    return train_inputOut, test_inputOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Generating pairs of 14x14 and sending to device. Here default\n",
    "N=1000\n",
    "(train_input,train_target,train_classes, \\\n",
    " test_input,test_target,test_classes) = prologue.generate_pair_sets(N)\n",
    "train_input = train_input.to(device)\n",
    "test_input = test_input.to(device)\n",
    "train_target = train_target.to(device)\n",
    "test_target = test_target.to(device)\n",
    "train_classes, test_classes = train_classes.to(device), test_classes.to(device)\n",
    "train_input,test_input = norm_(train_input,test_input);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model : Shared-weight ConvNet. Same as Plain ConvNet but just separating the inputs.\n",
    "\n",
    "`Conv layer 1 : takes 1x14x14 --> 32x12x12 --> Maxpool --> 32x6x6` \n",
    "\n",
    "`Conv Layer 2 : Takes 32x6x6 --> 64x4x4 --> Maxpool 64x2x2`\n",
    "\n",
    "`FC 1 : View(-1,64*2*2) --> 264 (random number but works)`\n",
    "\n",
    "`FC 2 : 264-->100 --> FC3 --> 2`\n",
    "\n",
    "`Using dropout, batchnorm on all hidden layers (FC1, FC2)`\n",
    "\n",
    "`Softmax as last activation, ReLU for all the others`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Conv layers, based on the filter size and layer sizes tested in explorationLeNetDual\n",
    "# must take 1x14x14 (so the same layers is used on both images)\n",
    "# Separated the modules because I couldn't figure out how to make a single net work...\n",
    "# will maybe merge into a single net later.\n",
    "class convlayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convlayer, self).__init__()\n",
    "        #self.conv1 : takes 1x14x14, gives 32x12x12, then maxpool(k=2) -> 32x6x6\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3) \n",
    "        #self.conv2 : takes 32x6x6, gives 64x4x4, then maxpool(k=2) -> outputs 64x2x2 to the fc layers\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2,stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2,stride=2))\n",
    "        return x\n",
    "#fc layers, adding a supp layer that has output dim 10 (instead of 2)\n",
    "#in order to maybe calculate an aux loss on this output to have classification?\n",
    "# REMEMBER TO ADD AN ACTIVATION WHEN CALLING SHARED_FCLAYER IN YOUR NETWORK!!\n",
    "# ex : F.relu(self.shared_fclayer(tmp1) or F.softmax(...,dim=1)\n",
    "class shared_fclayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(shared_fclayer,self).__init__()\n",
    "        #gets in 64x2x2, convers to 1x250\n",
    "        self.fc1 = nn.Linear(2*2*64,264)\n",
    "        self.bn1 = nn.BatchNorm1d(264)\n",
    "        #second layer : 250 to 100\n",
    "        self.fc2 = nn.Linear(264,100)  \n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "        #outputs dim 10 so we can test the aux loss for classifying numbers\n",
    "        #use softmax on fc3?\n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    def forward(self,x):\n",
    "        x = self.dropout(self.bn1(F.relu(self.fc1(x.view(-1,2*2*64)))))\n",
    "        x = self.dropout(self.bn2(F.relu(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "#extra final layer, not shared\n",
    "class final_predictionlayer(nn.Module):\n",
    "    #this final layer should take 2*10 (one for each image) and output 2 \n",
    "    def __init__(self):\n",
    "        super(final_predictionlayer,self).__init__()\n",
    "        self.final = nn.Linear(20,2)\n",
    "    def forward(self,x):\n",
    "        x = F.softmax(self.final(x),dim=1)\n",
    "        return x\n",
    "\n",
    "#weight-sharing Net\n",
    "#returns tmp1, tmp2 in order to calculate and optimize with auxLoss\n",
    "#Those will be compared to the train_classes.narrow(1,0,1) and .narrow(1,1,1)\n",
    "\n",
    "class AuxLossWS_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxLossWS_Net,self).__init__()\n",
    "        self.convlayer = convlayer()\n",
    "        self.fclayer = shared_fclayer()\n",
    "        self.final = final_predictionlayer()\n",
    "    def forward(self,x):\n",
    "        tmp1 = x.narrow(1,0,1) #viewing only one image\n",
    "        tmp2 = x.narrow(1,1,1) #viewing only one image\n",
    "        #applying the conv layers\n",
    "        tmp1 = self.convlayer.forward(tmp1) \n",
    "        tmp2 = self.convlayer.forward(tmp2)\n",
    "        #applying the fc layers\n",
    "        tmp1 = F.softmax(self.fclayer(tmp1),dim=1)\n",
    "        tmp2 = F.softmax(self.fclayer(tmp2),dim=1)\n",
    "        #viewing and final prediction\n",
    "        output = torch.cat((tmp1,tmp2),1)\n",
    "        output.view(-1,20)\n",
    "        x = self.final(output)\n",
    "        return x, tmp1, tmp2     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train_model, compute_nb_errors, run_net\n",
    "\n",
    "##### *Adapted to also train for auxLoss. Gamma determines the \"weight\" of the primary loss. Works best with gamma = 0.67, maybe try other values.*\n",
    "*run_net(...) does everything, use only this function, it will call the others. (See params)*\n",
    "\n",
    "*run_net(...) **RETURNS** the test error as a float, useful for average over multiple runs* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_aux(model, train_input, train_target, train_classes, nb_epochs=50, \n",
    "                eta=9e-2, mini_batch_size=25, gamma = 0.67,\n",
    "                    printTrain = False, graphLoss = False):\n",
    "    \"\"\"Trains the model, using CrossEntropyLoss and SGD \n",
    "    Model : Architecture to be tested, pytorch.nn.Module\n",
    "    Train_input : Input tensors Nx2x14x14, N = 1000\n",
    "    Train_target : Target labels, N, classes = 0 or 1\n",
    "    Nb_epochs : nb of epochs to train over\n",
    "    eta : Learning rate\n",
    "    mini_batch_size : Size of minibatch to be processed\"\"\"\n",
    "    \n",
    "    #Squeeze the classes labels (hotlabeling) for the auxLoss\n",
    "    trainlabel_1 = (train_classes.narrow(1,0,1)).squeeze()\n",
    "    trainlabel_2 = (train_classes.narrow(1,1,1)).squeeze()\n",
    "\n",
    "    model.train(True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(),lr=eta)\n",
    "    losses = []\n",
    "    for e in range(nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):    \n",
    "            \n",
    "            out_compare, out_1, out_2 = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            #Main Loss\n",
    "            loss_compare = criterion(out_compare, train_target.narrow(0, b, mini_batch_size))\n",
    "            #AuxLoss\n",
    "            loss_1 = criterion(out_1, trainlabel_1.narrow(0, b, mini_batch_size))\n",
    "            loss_2 = criterion(out_2, trainlabel_2.narrow(0, b, mini_batch_size))\n",
    "            #Weighted sum. Used to be Alpha*Loss1 + Beta*Loss2 + Gamma* Loss compare\n",
    "            #Didn't work well, try again with other alpha/betas < 1.\n",
    "            loss_sum = loss_1 + loss_2 + gamma*loss_compare\n",
    "            \n",
    "            losses.append(loss_sum)\n",
    "            model.zero_grad()\n",
    "            loss_sum.backward()\n",
    "            optimizer.step()\n",
    "        if printTrain : \n",
    "            print(\"Epoch : {} :: Train error : {}/{}, {:0f}%\".format(e,\n",
    "            compute_nb_errors(model,train_input,train_target,mini_batch_size),train_target.size(0),\n",
    "            (100*compute_nb_errors(model,train_input,train_target,mini_batch_size)/train_target.size(0))))\n",
    "    if graphLoss : \n",
    "        plt.plot(losses)\n",
    "        plt.ylabel('loss')\n",
    "#--------------------------------------------------------------------------------------------------------#\n",
    "def compute_nb_errors(model,data_input,data_target,mini_batch_size):\n",
    "    \"\"\"std from the séries\"\"\"\n",
    "    nb_errors = 0;\n",
    "    model.to(device)\n",
    "    data_input, data_target = data_input.to(device),data_target.to(device)\n",
    "    for b in range(0,data_input.size(0),mini_batch_size):\n",
    "        output, _, _ = model(data_input.narrow(0,b,mini_batch_size))\n",
    "        _, predicted_classes = torch.max(output, 1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b+k]!=predicted_classes[k]:\n",
    "                nb_errors += 1\n",
    "    \n",
    "    return nb_errors\n",
    "#--------------------------------------------------------------------------------------------------------#\n",
    "def run_net_aux(model,train_input,train_target, train_classes,\n",
    "            test_input, test_target, test_classes,\n",
    "            nb_epochs = 50,eta=9e-2,mini_batch_size=25,gamma=0.67,\n",
    "            printTrain = False,graphLoss = False):\n",
    "    \"\"\"\"\"\"\n",
    "    model.to(device)\n",
    "    print(\"Model tested : {}\".format(str(model)[:str(model).find('(')]))\n",
    "    print(\"\"\"Using {} epochs, lr = {:.04f},Mini batch size = {}\"\"\".format(nb_epochs,\n",
    "                                                                          eta,mini_batch_size))\n",
    "    train_model_aux(model, train_input, train_target, train_classes,\n",
    "                nb_epochs, eta, mini_batch_size, gamma, printTrain,graphLoss)\n",
    "    model.train(False)\n",
    "    train_error = compute_nb_errors(model, train_input, train_target,mini_batch_size) / train_input.size(0) * 100\n",
    "    test_error = compute_nb_errors(model, test_input, test_target,mini_batch_size) / test_input.size(0) * 100\n",
    "    print('train_error {:.02f}% test_error {:.02f}% \\n'.format(\n",
    "                train_error,\n",
    "                test_error\n",
    "            )\n",
    "        )\n",
    "    return float(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
