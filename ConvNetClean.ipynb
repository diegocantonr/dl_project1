{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned up basic LeNet exploration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Starting points : Importing, getting data, normalizing*\n",
    "`Stuff you need, but is hidden here.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using : cuda\n"
     ]
    }
   ],
   "source": [
    "#Starting points : import everything, use cuda if available\n",
    "import torch\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue as prologue\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using : {}\".format(device))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using : {}\".format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def norm_(train_input,test_input):\n",
    "    \"\"\"Function to normalize the input --> done IN PLACE!\"\"\"\n",
    "    mu, std = train_input.mean(), train_input.std()\n",
    "    train_inputOut = train_input.sub_(mu).div_(std)\n",
    "    test_inputOut = test_input.sub_(mu).div_(std)\n",
    "    return train_inputOut, test_inputOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Generating pairs of 14x14 and sending to device. Here default\n",
    "N=1000\n",
    "(train_input,train_target,train_classes, \\\n",
    " test_input,test_target,test_classes) = prologue.generate_pair_sets(N)\n",
    "train_input = train_input.to(device)\n",
    "test_input = test_input.to(device)\n",
    "train_target = train_target.to(device)\n",
    "test_target = test_target.to(device)\n",
    "train_classes, test_classes = train_classes.to(device), test_classes.to(device)\n",
    "train_input,test_input = norm_(train_input,test_input);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model : Convnet\n",
    "\n",
    "`Conv layer 1 : takes 2x14x14 --> 32x12x12 --> Maxpool --> 32x6x6` \n",
    "\n",
    "`Conv Layer 2 : Takes 32x6x6 --> 64x4x4 --> Maxpool 64x2x2`\n",
    "\n",
    "`FC 1 : View(-1,64*2*2) --> 264 (random number but works)`\n",
    "\n",
    "`FC 2 : 264-->100 --> FC3 --> 2`\n",
    "\n",
    "`Using dropout, batchnorm on all hidden layers (FC1, FC2)`\n",
    "\n",
    "`Softmax as last activation, ReLU for all the others`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model\n",
    "class PlainConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PlainConvNet, self).__init__()\n",
    "        #takes 2x14x14\n",
    "        #gives 32x12x12, 32x6x6 after maxpool\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3) \n",
    "        #takes 32x6x6, gives 64x4x4 then 64x2x2 after maxpool\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        #takes 64*2*2 = 256, gives nb_hidden\n",
    "        self.fc1 = nn.Linear(2*2*64, 250)\n",
    "        self.bn1 = nn.BatchNorm1d(250)\n",
    "        #takes nb_hidden, gives 10 classes\n",
    "        self.fc2 = nn.Linear(250,100)\n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "        self.fc3 = nn.Linear(100, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2,stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2,stride=2))\n",
    "        x = self.dropout(self.bn1(F.relu(self.fc1(x.view(-1, 2*2 * 64)))))\n",
    "        x = self.dropout(self.bn2(F.relu(self.fc2(x))))\n",
    "        x = F.softmax(self.fc3(x),dim=1)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train_model, compute_nb_errors, run_net\n",
    "\n",
    "All nets run with : 50 epoch, eta = 9e-2, mini_batch_size = 25 by default. \n",
    "\n",
    "Maybe use smaller epochs to run faster (35-40)\n",
    "\n",
    "\n",
    "*run_net(...) does everything, use only this function, it will call the others. (See params)*\n",
    "\n",
    "*run_net(...) **RETURNS** the test error as a float, useful for average over multiple runs* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, nb_epochs=50, \n",
    "                eta=9e-2, mini_batch_size=25,printTrain = False, graphLoss = False):\n",
    "    \"\"\"Trains the model, using CrossEntropyLoss and SGD \n",
    "    Model : Architecture to be tested, pytorch.nn.Module\n",
    "    Train_input : Input tensors Nx2x14x14, N = 1000\n",
    "    Train_target : Target labels, N, classes = 0 or 1\n",
    "    Nb_epochs : nb of epochs to train over\n",
    "    eta : Learning rate\n",
    "    mini_batch_size : Size of minibatch to be processed\"\"\"\n",
    "    model.train(True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(),lr=eta)\n",
    "    losses = []\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):    \n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            losses.append(loss)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if printTrain : \n",
    "            print(\"Epoch : {} :: Train error : {}/{}, {:0f}%\".format(e,\n",
    "            compute_nb_errors(model,train_input,train_target,mini_batch_size),train_target.size(0),\n",
    "            (100*compute_nb_errors(model,train_input,train_target,mini_batch_size)/train_target.size(0))))\n",
    "    if graphLoss : \n",
    "        plt.plot(losses)\n",
    "        plt.ylabel('loss')\n",
    "#--------------------------------------------------------------------------------------------------------#\n",
    "def compute_nb_errors(model,data_input,data_target,mini_batch_size):\n",
    "    \"\"\"std from the s√©ries\"\"\"\n",
    "    nb_errors = 0;\n",
    "    model.to(device)\n",
    "    data_input, data_target = data_input.to(device),data_target.to(device)\n",
    "    for b in range(0,data_input.size(0),mini_batch_size):\n",
    "        output = model(data_input.narrow(0,b,mini_batch_size))\n",
    "        _, predicted_classes = torch.max(output, 1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b+k]!=predicted_classes[k]:\n",
    "                nb_errors += 1\n",
    "    \n",
    "    return nb_errors\n",
    "#--------------------------------------------------------------------------------------------------------#\n",
    "def run_net(model,train_input,train_target,\n",
    "            test_input, test_target,\n",
    "            nb_epochs = 50,eta=9e-2,mini_batch_size=25,\n",
    "            printTrain = False,graphLoss = False):\n",
    "    \"\"\"\"\"\"\n",
    "    model.to(device)\n",
    "    print(\"Model tested : {}\".format(str(model)[:str(model).find('(')]))\n",
    "    print(\"\"\"Using {} epochs, lr = {:.04f},Mini batch size = {}\"\"\".format(nb_epochs,\n",
    "                                                                          eta,mini_batch_size))\n",
    "    train_model(model, train_input, train_target,\n",
    "                nb_epochs, eta, mini_batch_size,printTrain,graphLoss)\n",
    "    model.train(False)\n",
    "    train_error = compute_nb_errors(model, train_input, train_target,mini_batch_size) / train_input.size(0) * 100\n",
    "    test_error = compute_nb_errors(model, test_input, test_target,mini_batch_size) / test_input.size(0) * 100\n",
    "    print('train_error {:.02f}% test_error {:.02f}% \\n'.format(\n",
    "                train_error,\n",
    "                test_error\n",
    "            )\n",
    "        )\n",
    "    return float(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
